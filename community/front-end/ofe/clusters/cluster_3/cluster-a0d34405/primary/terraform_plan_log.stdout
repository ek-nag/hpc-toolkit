module.slurm_controller.module.slurm_controller_instance.data.local_file.slurm_conf_tpl: Reading...
module.partition_0.module.slurm_partition.module.reconfigure_placement_groups[0].data.local_file.destroy_resource_policies: Reading...
module.slurm_controller.module.slurm_controller_instance.data.local_file.slurmdbd_conf_tpl: Reading...
module.slurm_controller.module.slurm_controller_instance.module.cleanup_subscriptions[0].data.local_file.destroy_subscriptions: Reading...
module.slurm_controller.module.slurm_controller_template.data.local_file.startup: Reading...
module.slurm_login.module.slurm_login_template.data.local_file.startup: Reading...
module.slurm_controller.module.slurm_controller_instance.module.reconfigure_critical[0].data.local_file.destroy_nodes: Reading...
module.slurm_controller.module.slurm_controller_instance.data.local_file.cgroup_conf_tpl: Reading...
module.slurm_controller.module.slurm_controller_instance.data.local_file.slurmdbd_conf_tpl: Read complete after 0s [id=45039ec29ef691f55da128333837adcb28bd19a0]
module.slurm_controller.module.slurm_controller_instance.module.reconfigure_critical[0].data.local_file.destroy_nodes: Read complete after 0s [id=98a2faef60120e7350ccb2ba9e85cd22839e907d]
module.slurm_controller.module.slurm_controller_instance.data.local_file.slurm_conf_tpl: Read complete after 0s [id=c96fdbdd36600006a3bc33db33fd9a2f356ac215]
module.slurm_controller.module.slurm_controller_instance.data.local_file.cgroup_conf_tpl: Read complete after 0s [id=fe856d7ed3738c1cc82a2e7bca11f65de71f1e3b]
module.partition_0.module.slurm_partition.module.reconfigure_placement_groups[0].data.local_file.destroy_resource_policies: Read complete after 0s [id=17f4a9a5082fa2f97904ecf42ed58b42a8e8ddb1]
module.slurm_controller.module.slurm_controller_instance.module.cleanup_subscriptions[0].data.local_file.destroy_subscriptions: Read complete after 0s [id=683606ea186658dd74ff9c27bfce60a3d555df8f]
module.slurm_controller.module.slurm_controller_template.data.local_file.startup: Read complete after 0s [id=88f00230a444d6dcbdafde1b1fe118effdccc8b7]
module.slurm_login.module.slurm_login_template.data.local_file.startup: Read complete after 0s [id=88f00230a444d6dcbdafde1b1fe118effdccc8b7]
module.hpc_network.data.google_compute_network.vpc: Reading...
module.slurm_controller.data.google_compute_default_service_account.default: Reading...
module.slurm_login.data.google_compute_default_service_account.default: Reading...
module.hpc_network.data.google_compute_subnetwork.primary_subnetwork: Reading...
module.partition_0.data.google_compute_zones.available: Reading...
module.partition_0-group.data.google_compute_default_service_account.default: Reading...
module.slurm_login.data.google_compute_image.slurm: Reading...
module.partition_0-group.data.google_compute_image.slurm: Reading...
module.partition_0.module.slurm_partition.data.google_compute_subnetwork.partition_subnetwork: Reading...
module.slurm_controller.data.google_compute_image.slurm: Reading...
module.hpc_network.data.google_compute_subnetwork.primary_subnetwork: Read complete after 0s [id=projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2]
module.partition_0.module.slurm_partition.data.google_compute_subnetwork.partition_subnetwork: Read complete after 0s [id=projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2]
module.hpc_network.data.google_compute_network.vpc: Read complete after 0s [id=projects/eimantask-personal-project/global/networks/legible-polliwog-network]
module.partition_0.data.google_compute_zones.available: Read complete after 0s [id=projects/eimantask-personal-project/regions/us-east1]
module.slurm_login.data.google_compute_image.slurm: Read complete after 0s [id=projects/schedmd-slurm-public/global/images/slurm-gcp-5-10-hpc-centos-7-1707957594]
module.partition_0-group.data.google_compute_image.slurm: Read complete after 0s [id=projects/schedmd-slurm-public/global/images/slurm-gcp-5-10-hpc-centos-7-1707957594]
module.slurm_controller.data.google_compute_image.slurm: Read complete after 0s [id=projects/schedmd-slurm-public/global/images/slurm-gcp-5-10-hpc-centos-7-1707957594]
module.partition_0-group.data.google_compute_default_service_account.default: Read complete after 0s [id=projects/eimantask-personal-project/serviceAccounts/260846834422-compute@developer.gserviceaccount.com]
module.partition_0.module.slurm_partition.module.slurm_compute_template["ghpc"].data.local_file.startup: Reading...
module.partition_0.module.slurm_partition.module.slurm_compute_template["ghpc"].data.local_file.startup: Read complete after 0s [id=88f00230a444d6dcbdafde1b1fe118effdccc8b7]
module.slurm_controller.data.google_compute_default_service_account.default: Read complete after 0s [id=projects/eimantask-personal-project/serviceAccounts/260846834422-compute@developer.gserviceaccount.com]
module.slurm_login.data.google_compute_default_service_account.default: Read complete after 0s [id=projects/eimantask-personal-project/serviceAccounts/260846834422-compute@developer.gserviceaccount.com]

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  + create
 <= read (data resources)

Terraform will perform the following actions:

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/compute.instanceAdmin.v1"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/compute.instanceAdmin.v1"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/compute.networkAdmin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/compute.networkAdmin"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/compute.securityAdmin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/compute.securityAdmin"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/iam.serviceAccountAdmin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/iam.serviceAccountAdmin"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/iam.serviceAccountUser"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/iam.serviceAccountUser"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/logging.logWriter"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/logging.logWriter"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/monitoring.metricWriter"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/monitoring.metricWriter"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/pubsub.admin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/pubsub.admin"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/resourcemanager.projectIamAdmin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/resourcemanager.projectIamAdmin"
    }

  # module.hpc_service_account.module.service_account.google_project_iam_member.project-roles["sa-eimantask-personal-project=>roles/storage.objectAdmin"] will be created
  + resource "google_project_iam_member" "project-roles" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/storage.objectAdmin"
    }

  # module.hpc_service_account.module.service_account.google_service_account.service_accounts["sa"] will be created
  + resource "google_service_account" "service_accounts" {
      + account_id   = "cluster-a0d34405-sa"
      + description  = "Service Account (cluster-a0d34405)"
      + disabled     = false
      + display_name = "Service Account (cluster-a0d34405)"
      + email        = (known after apply)
      + id           = (known after apply)
      + member       = (known after apply)
      + name         = (known after apply)
      + project      = "eimantask-personal-project"
      + unique_id    = (known after apply)
    }

  # module.partition_0.module.slurm_partition.data.google_compute_instance_template.group_template["ghpc"] will be read during apply
  # (config refers to values not yet known)
 <= data "google_compute_instance_template" "group_template" {
      + advanced_machine_features    = (known after apply)
      + can_ip_forward               = (known after apply)
      + confidential_instance_config = (known after apply)
      + description                  = (known after apply)
      + disk                         = (known after apply)
      + guest_accelerator            = (known after apply)
      + id                           = (known after apply)
      + instance_description         = (known after apply)
      + labels                       = (known after apply)
      + machine_type                 = (known after apply)
      + metadata                     = (known after apply)
      + metadata_fingerprint         = (known after apply)
      + metadata_startup_script      = (known after apply)
      + min_cpu_platform             = (known after apply)
      + name                         = (known after apply)
      + name_prefix                  = (known after apply)
      + network_interface            = (known after apply)
      + network_performance_config   = (known after apply)
      + project                      = "eimantask-personal-project"
      + region                       = (known after apply)
      + reservation_affinity         = (known after apply)
      + resource_policies            = (known after apply)
      + scheduling                   = (known after apply)
      + self_link                    = (known after apply)
      + service_account              = (known after apply)
      + shielded_instance_config     = (known after apply)
      + tags                         = (known after apply)
      + tags_fingerprint             = (known after apply)
    }

  # module.partition_0.module.slurm_partition.google_compute_project_metadata_item.partition_startup_scripts["ghpc_startup_sh"] will be created
  + resource "google_compute_project_metadata_item" "partition_startup_scripts" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-partition-batch-script-ghpc_startup_sh"
      + project = "eimantask-personal-project"

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.partition_0.module.slurm_partition.null_resource.partition will be created
  + resource "null_resource" "partition" {
      + id       = (known after apply)
      + triggers = {
          + "partition" = (known after apply)
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.data.google_compute_instance_template.controller_template[0] will be read during apply
  # (config refers to values not yet known)
 <= data "google_compute_instance_template" "controller_template" {
      + advanced_machine_features    = (known after apply)
      + can_ip_forward               = (known after apply)
      + confidential_instance_config = (known after apply)
      + description                  = (known after apply)
      + disk                         = (known after apply)
      + guest_accelerator            = (known after apply)
      + id                           = (known after apply)
      + instance_description         = (known after apply)
      + labels                       = (known after apply)
      + machine_type                 = (known after apply)
      + metadata                     = (known after apply)
      + metadata_fingerprint         = (known after apply)
      + metadata_startup_script      = (known after apply)
      + min_cpu_platform             = (known after apply)
      + name                         = (known after apply)
      + name_prefix                  = (known after apply)
      + network_interface            = (known after apply)
      + network_performance_config   = (known after apply)
      + region                       = (known after apply)
      + reservation_affinity         = (known after apply)
      + resource_policies            = (known after apply)
      + scheduling                   = (known after apply)
      + self_link                    = (known after apply)
      + service_account              = (known after apply)
      + shielded_instance_config     = (known after apply)
      + tags                         = (known after apply)
      + tags_fingerprint             = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.cgroup_conf will be created
  + resource "google_compute_project_metadata_item" "cgroup_conf" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-tpl-cgroup-conf"
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            # cgroup.conf
            # https://slurm.schedmd.com/cgroup.conf.html
            
            CgroupAutomount=no
            #CgroupMountpoint=/sys/fs/cgroup
            ConstrainCores=yes
            ConstrainRamSpace=yes
            ConstrainSwapSpace=no
            ConstrainDevices=yes
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.compute_startup_scripts["ghpc_startup_sh"] will be created
  + resource "google_compute_project_metadata_item" "compute_startup_scripts" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-compute-script-ghpc_startup_sh"
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            #!/bin/bash
            gsutil cp gs://aicluster-storage-08c2/clusters/3/bootstrap_compute.sh - | bash
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.config will be created
  + resource "google_compute_project_metadata_item" "config" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-config"
      + project = "eimantask-personal-project"
      + value   = (known after apply)

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.controller_startup_scripts["ghpc_startup_sh"] will be created
  + resource "google_compute_project_metadata_item" "controller_startup_scripts" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-controller-script-ghpc_startup_sh"
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            #!/bin/bash
            echo "******************************************** CALLING CONTROLLER STARTUP"
            gsutil cp gs://aicluster-storage-08c2/clusters/3/bootstrap_controller.sh - | bash
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.slurm_conf will be created
  + resource "google_compute_project_metadata_item" "slurm_conf" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-tpl-slurm-conf"
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            # slurm.conf
            # https://slurm.schedmd.com/slurm.conf.html
            # https://slurm.schedmd.com/configurator.html
            
            ProctrackType=proctrack/cgroup
            SlurmctldPidFile=/var/run/slurm/slurmctld.pid
            SlurmdPidFile=/var/run/slurm/slurmd.pid
            TaskPlugin=task/affinity,task/cgroup
            MaxNodeCount=64000
            
            #
            #
            # SCHEDULING
            SchedulerType=sched/backfill
            SelectType=select/cons_tres
            SelectTypeParameters=CR_Core_Memory
            
            #
            #
            # LOGGING AND ACCOUNTING
            AccountingStoreFlags=job_comment
            JobAcctGatherFrequency=30
            JobAcctGatherType=jobacct_gather/cgroup
            SlurmctldDebug=info
            SlurmdDebug=info
            DebugFlags=Power
            
            #
            #
            # TIMERS
            MessageTimeout=60
            
            ################################################################################
            #              vvvvv  WARNING: DO NOT MODIFY SECTION BELOW  vvvvv              #
            ################################################################################
            
            SlurmctldHost={control_host}({control_addr})
            
            AuthType=auth/munge
            AuthInfo=cred_expire=120
            AuthAltTypes=auth/jwt
            CredType=cred/munge
            MpiDefault={mpi_default}
            ReturnToService=2
            SlurmctldPort={control_host_port}
            SlurmdPort=6818
            SlurmdSpoolDir=/var/spool/slurmd
            SlurmUser=slurm
            StateSaveLocation={state_save}
            
            #
            #
            # LOGGING AND ACCOUNTING
            AccountingStorageType=accounting_storage/slurmdbd
            AccountingStorageHost={control_host}
            ClusterName={name}
            SlurmctldLogFile={slurmlog}/slurmctld.log
            SlurmdLogFile={slurmlog}/slurmd-%n.log
            
            #
            #
            # GENERATED CLOUD CONFIGURATIONS
            include cloud.conf
            
            ################################################################################
            #              ^^^^^  WARNING: DO NOT MODIFY SECTION ABOVE  ^^^^^              #
            ################################################################################
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.slurmdbd_conf will be created
  + resource "google_compute_project_metadata_item" "slurmdbd_conf" {
      + id      = (known after apply)
      + key     = "clustera0d-slurm-tpl-slurmdbd-conf"
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            # slurmdbd.conf
            # https://slurm.schedmd.com/slurmdbd.conf.html
            
            DebugLevel=info
            PidFile=/var/run/slurm/slurmdbd.pid
            
            ################################################################################
            #              vvvvv  WARNING: DO NOT MODIFY SECTION BELOW  vvvvv              #
            ################################################################################
            
            AuthType=auth/munge
            AuthAltTypes=auth/jwt
            AuthAltParameters=jwt_key={state_save}/jwt_hs256.key
            
            DbdHost={control_host}
            
            LogFile={slurmlog}/slurmdbd.log
            
            SlurmUser=slurm
            
            StorageLoc={db_name}
            
            StorageType=accounting_storage/mysql
            StorageHost={db_host}
            StoragePort={db_port}
            StorageUser={db_user}
            StoragePass={db_pass}
            
            ################################################################################
            #              ^^^^^  WARNING: DO NOT MODIFY SECTION ABOVE  ^^^^^              #
            ################################################################################
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_pubsub_schema.this[0] will be created
  + resource "google_pubsub_schema" "this" {
      + definition = <<-EOT
            syntax = "proto3";
            message Results {
              string request = 1;
              string timestamp = 2;
            }
        EOT
      + id         = (known after apply)
      + name       = "clustera0d-slurm-events"
      + project    = (known after apply)
      + type       = "PROTOCOL_BUFFER"
    }

  # module.slurm_controller.module.slurm_controller_instance.google_pubsub_topic.this[0] will be created
  + resource "google_pubsub_topic" "this" {
      + id      = (known after apply)
      + labels  = {
          + "slurm_cluster_name" = "clustera0d"
        }
      + name    = (known after apply)
      + project = (known after apply)

      + schema_settings {
          + encoding = "JSON"
          + schema   = (known after apply)
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.google_pubsub_topic_iam_member.topic_publisher[0] will be created
  + resource "google_pubsub_topic_iam_member" "topic_publisher" {
      + etag    = (known after apply)
      + id      = (known after apply)
      + member  = (known after apply)
      + project = "eimantask-personal-project"
      + role    = "roles/pubsub.publisher"
      + topic   = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.random_string.topic_suffix will be created
  + resource "random_string" "topic_suffix" {
      + id          = (known after apply)
      + length      = 8
      + lower       = true
      + min_lower   = 0
      + min_numeric = 0
      + min_special = 0
      + min_upper   = 0
      + number      = true
      + numeric     = true
      + result      = (known after apply)
      + special     = false
      + upper       = true
    }

  # module.slurm_controller.module.slurm_controller_instance.random_uuid.cluster_id will be created
  + resource "random_uuid" "cluster_id" {
      + id     = (known after apply)
      + result = (known after apply)
    }

  # module.slurm_login.module.slurm_login_instance.google_compute_project_metadata_item.login_startup_scripts["ghpc_startup_sh"] will be created
  + resource "google_compute_project_metadata_item" "login_startup_scripts" {
      + id      = (known after apply)
      + key     = (known after apply)
      + project = "eimantask-personal-project"
      + value   = <<-EOT
            #!/bin/bash
            echo "******************************************** CALLING LOGIN STARTUP"
            gsutil cp gs://aicluster-storage-08c2/clusters/3/bootstrap_login.sh - | bash
        EOT

      + timeouts {
          + create = "10m"
          + delete = "10m"
          + update = "10m"
        }
    }

  # module.slurm_login.module.slurm_login_instance.random_string.suffix will be created
  + resource "random_string" "suffix" {
      + id          = (known after apply)
      + length      = 8
      + lower       = true
      + min_lower   = 0
      + min_numeric = 0
      + min_special = 0
      + min_upper   = 0
      + number      = true
      + numeric     = true
      + result      = (known after apply)
      + special     = false
      + upper       = false
    }

  # module.partition_0.module.slurm_partition.module.reconfigure_critical[0].data.local_file.destroy_nodes will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "destroy_nodes" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts/destroy_nodes.py"
      + id                   = (known after apply)
    }

  # module.partition_0.module.slurm_partition.module.reconfigure_critical[0].null_resource.destroy_nodes_on_create[0] will be created
  + resource "null_resource" "destroy_nodes_on_create" {
      + id       = (known after apply)
      + triggers = {
          + "enable_placement_groups"                   = "false"
          + "partition_startup_scripts_ghpc_startup_sh" = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"
          + "project_id"                                = "eimantask-personal-project"
          + "script_path"                               = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts/destroy_nodes.py"
          + "scripts_dir"                               = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts"
          + "slurm_cluster_name"                        = "clustera0d"
          + "subnetwork"                                = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2"
        }
    }

  # module.partition_0.module.slurm_partition.module.reconfigure_node_groups["ghpc"].data.local_file.destroy_nodes will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "destroy_nodes" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts/destroy_nodes.py"
      + id                   = (known after apply)
    }

  # module.partition_0.module.slurm_partition.module.reconfigure_node_groups["ghpc"].null_resource.destroy_nodes_on_create[0] will be created
  + resource "null_resource" "destroy_nodes_on_create" {
      + id       = (known after apply)
      + triggers = {
          + "bandwidth_tier"       = "platform_default"
          + "instance_template"    = (known after apply)
          + "project_id"           = "eimantask-personal-project"
          + "script_path"          = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts/destroy_nodes.py"
          + "scripts_dir"          = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts"
          + "slurm_cluster_name"   = "clustera0d"
          + "spot_instance_config" = null
        }
    }

  # module.partition_0.module.slurm_partition.module.reconfigure_placement_groups[0].null_resource.destroy_resource_policies_on_create[0] will be created
  + resource "null_resource" "destroy_resource_policies_on_create" {
      + id       = (known after apply)
      + triggers = {
          + "enable_placement_groups" = "false"
          + "partition_name"          = "batch"
          + "project_id"              = "eimantask-personal-project"
          + "script_path"             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts/destroy_resource_policies.py"
          + "scripts_dir"             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/partition_0.slurm_partition/scripts"
          + "slurm_cluster_name"      = "clustera0d"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.cleanup_subscriptions[0].null_resource.destroy_subscriptions_on_destroy[0] will be created
  + resource "null_resource" "destroy_subscriptions_on_destroy" {
      + id       = (known after apply)
      + triggers = {
          + "script_path"        = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/destroy_subscriptions.py"
          + "scripts_dir"        = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts"
          + "slurm_cluster_name" = "clustera0d"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.reconfigure_critical[0].null_resource.destroy_nodes_on_create[0] will be created
  + resource "null_resource" "destroy_nodes_on_create" {
      + id       = (known after apply)
      + triggers = {
          + "compute_d_ghpc_startup_sh" = "ffd2254a3b089aed14885edb58d0769b5becf7f51c2b2b67f997762267a52ee6"
          + "controller_id"             = (known after apply)
          + "project_id"                = "eimantask-personal-project"
          + "script_path"               = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/destroy_nodes.py"
          + "scripts_dir"               = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts"
          + "slurm_cluster_name"        = "clustera0d"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.reconfigure_notify[0].data.local_file.notify_cluster will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "notify_cluster" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/notify_cluster.py"
      + id                   = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.module.reconfigure_notify[0].null_resource.notify_cluster will be created
  + resource "null_resource" "notify_cluster" {
      + id       = (known after apply)
      + triggers = {
          + "cgroup_conf"   = "b88ac215e7c31ee56a224b1a87b15704b388f291c9fab3f9cc217ea95345c60a"
          + "compute_list"  = "clustera0d-batch-ghpc-0,clustera0d-batch-ghpc-1,clustera0d-batch-ghpc-2,clustera0d-batch-ghpc-3"
          + "config"        = (known after apply)
          + "project_id"    = "eimantask-personal-project"
          + "script_path"   = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/notify_cluster.py"
          + "scripts_dir"   = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts"
          + "slurm_conf"    = "af711004d945f8c74376300d9a9b4ae9188246893507f29973a9d503af195c22"
          + "slurmdbd_conf" = "d6471fd70aa2372012a16043209f18aa052ad2aa18aa9271cd7889cb44695226"
          + "topic"         = (known after apply)
          + "type"          = "reconfig"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.reconfigure_partitions[0].data.local_file.destroy_nodes will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "destroy_nodes" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/destroy_nodes.py"
      + id                   = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.module.reconfigure_partitions[0].null_resource.destroy_nodes_on_create[0] will be created
  + resource "null_resource" "destroy_nodes_on_create" {
      + id       = (known after apply)
      + triggers = {
          + "compute_list"       = "clustera0d-batch-ghpc-0,clustera0d-batch-ghpc-1,clustera0d-batch-ghpc-2,clustera0d-batch-ghpc-3"
          + "project_id"         = "eimantask-personal-project"
          + "script_path"        = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/destroy_nodes.py"
          + "scripts_dir"        = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts"
          + "slurm_cluster_name" = "clustera0d"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.data.google_compute_instance_template.base will be read during apply
  # (config refers to values not yet known)
 <= data "google_compute_instance_template" "base" {
      + advanced_machine_features    = (known after apply)
      + can_ip_forward               = (known after apply)
      + confidential_instance_config = (known after apply)
      + description                  = (known after apply)
      + disk                         = (known after apply)
      + guest_accelerator            = (known after apply)
      + id                           = (known after apply)
      + instance_description         = (known after apply)
      + labels                       = (known after apply)
      + machine_type                 = (known after apply)
      + metadata                     = (known after apply)
      + metadata_fingerprint         = (known after apply)
      + metadata_startup_script      = (known after apply)
      + min_cpu_platform             = (known after apply)
      + name                         = (known after apply)
      + name_prefix                  = (known after apply)
      + network_interface            = (known after apply)
      + network_performance_config   = (known after apply)
      + project                      = "eimantask-personal-project"
      + region                       = (known after apply)
      + reservation_affinity         = (known after apply)
      + resource_policies            = (known after apply)
      + scheduling                   = (known after apply)
      + self_link                    = (known after apply)
      + service_account              = (known after apply)
      + shielded_instance_config     = (known after apply)
      + tags                         = (known after apply)
      + tags_fingerprint             = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.data.google_compute_zones.available will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "google_compute_zones" "available" {
      + id      = (known after apply)
      + names   = (known after apply)
      + project = "eimantask-personal-project"
      + region  = "us-east1"
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.data.local_file.startup will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "startup" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_controller.slurm_controller_instance/scripts/startup.sh"
      + id                   = (known after apply)
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.google_compute_instance_from_template.slurm_instance[0] will be created
  + resource "google_compute_instance_from_template" "slurm_instance" {
      + allow_stopping_for_update = true
      + attached_disk             = (known after apply)
      + can_ip_forward            = (known after apply)
      + cpu_platform              = (known after apply)
      + current_status            = (known after apply)
      + deletion_protection       = (known after apply)
      + description               = (known after apply)
      + desired_status            = (known after apply)
      + enable_display            = (known after apply)
      + guest_accelerator         = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + instance_id               = (known after apply)
      + label_fingerprint         = (known after apply)
      + labels                    = (known after apply)
      + machine_type              = (known after apply)
      + metadata                  = (known after apply)
      + metadata_fingerprint      = (known after apply)
      + metadata_startup_script   = (known after apply)
      + min_cpu_platform          = (known after apply)
      + name                      = "clustera0d-controller"
      + project                   = "eimantask-personal-project"
      + resource_policies         = (known after apply)
      + scratch_disk              = (known after apply)
      + self_link                 = (known after apply)
      + service_account           = (known after apply)
      + source_instance_template  = (known after apply)
      + tags                      = (known after apply)
      + tags_fingerprint          = (known after apply)
      + zone                      = "us-east1-b"

      + network_interface {
          + access_config               = (known after apply)
          + alias_ip_range              = (known after apply)
          + internal_ipv6_prefix_length = (known after apply)
          + ipv6_access_type            = (known after apply)
          + ipv6_address                = (known after apply)
          + name                        = (known after apply)
          + network                     = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/global/networks/legible-polliwog-network"
          + network_ip                  = (known after apply)
          + nic_type                    = (known after apply)
          + queue_count                 = (known after apply)
          + stack_type                  = (known after apply)
          + subnetwork                  = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2"
          + subnetwork_project          = (known after apply)
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.google_pubsub_subscription.pull_subscriptions[0] will be created
  + resource "google_pubsub_subscription" "pull_subscriptions" {
      + ack_deadline_seconds       = 120
      + enable_message_ordering    = true
      + id                         = (known after apply)
      + labels                     = {
          + "slurm_cluster_name" = "clustera0d"
        }
      + message_retention_duration = "604800s"
      + name                       = "clustera0d-controller"
      + project                    = "eimantask-personal-project"
      + topic                      = (known after apply)

      + retry_policy {
          + maximum_backoff = "300s"
          + minimum_backoff = "30s"
        }
    }

  # module.slurm_controller.module.slurm_controller_instance.module.slurm_controller_instance.google_pubsub_subscription_iam_member.pull_subscription_binding[0] will be created
  + resource "google_pubsub_subscription_iam_member" "pull_subscription_binding" {
      + etag         = (known after apply)
      + id           = (known after apply)
      + member       = (known after apply)
      + project      = "eimantask-personal-project"
      + role         = "roles/pubsub.subscriber"
      + subscription = "clustera0d-controller"
    }

  # module.slurm_controller.module.slurm_controller_template.module.instance_template.google_compute_instance_template.tpl will be created
  + resource "google_compute_instance_template" "tpl" {
      + can_ip_forward          = false
      + id                      = (known after apply)
      + labels                  = {
          + "created_by"          = "aicluster-server"
          + "ghpc_blueprint"      = "cluster-a0d34405"
          + "ghpc_deployment"     = "cluster-a0d34405"
          + "ghpc_module"         = "schedmd-slurm-gcp-v5-controller"
          + "ghpc_role"           = "scheduler"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "controller"
        }
      + machine_type            = "n2-standard-2"
      + metadata                = {
          + "VmDnsSetting"        = "GlobalOnly"
          + "enable-oslogin"      = "TRUE"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "controller"
        }
      + metadata_fingerprint    = (known after apply)
      + metadata_startup_script = <<-EOT
            #!/bin/bash
            # Copyright (C) SchedMD LLC.
            #
            # Licensed under the Apache License, Version 2.0 (the "License");
            # you may not use this file except in compliance with the License.
            # You may obtain a copy of the License at
            #
            #     http://www.apache.org/licenses/LICENSE-2.0
            #
            # Unless required by applicable law or agreed to in writing, software
            # distributed under the License is distributed on an "AS IS" BASIS,
            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            # See the License for the specific language governing permissions and
            # limitations under the License.
            
            set -e
            
            SLURM_DIR=/slurm
            FLAGFILE=$SLURM_DIR/slurm_configured_do_not_remove
            SCRIPTS_DIR=$SLURM_DIR/scripts
            
            METADATA_SERVER="metadata.google.internal"
            URL="http://$METADATA_SERVER/computeMetadata/v1"
            HEADER="Metadata-Flavor:Google"
            CURL="curl -sS --fail --header $HEADER"
            
            function fetch_scripts {
            	# fetch project metadata
            	if ! CLUSTER=$($CURL $URL/instance/attributes/slurm_cluster_name); then
            		echo "ERROR: cluster name not found in instance metadata. Quitting!"
            		return 1
            	fi
            	if ! META_DEVEL=$($CURL $URL/project/attributes/$CLUSTER-slurm-devel); then
            		echo "WARNING: $CLUSTER-slurm-devel not found in project metadata, skipping script update"
            		return
            	fi
            	echo devel data found in project metadata, looking to update scripts
            	if STARTUP_SCRIPT=$(jq -re '."startup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating startup.sh from project metadata"
            		printf '%s' "$STARTUP_SCRIPT" > $STARTUP_SCRIPT_FILE
            	else
            		echo "WARNING: startup-script not found in project metadata, skipping update"
            	fi
            	if SETUP_SCRIPT=$(jq -re '."setup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating setup.py from project metadata"
            		printf '%s' "$SETUP_SCRIPT" > $SETUP_SCRIPT_FILE
            	else
            		echo "WARNING: setup-script not found in project metadata, skipping update"
            	fi
            	if UTIL_SCRIPT=$(jq -re '."util-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating util.py from project metadata"
            		printf '%s' "$UTIL_SCRIPT" > $UTIL_SCRIPT_FILE
            	else
            		echo "WARNING: util-script not found in project metadata, skipping update"
            	fi
            	if RESUME_SCRIPT=$(jq -re '."slurm-resume"' <<< "$META_DEVEL"); then
            		echo "INFO: updating resume.py from project metadata"
            		printf '%s' "$RESUME_SCRIPT" > $RESUME_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-resume not found in project metadata, skipping update"
            	fi
            	if SUSPEND_SCRIPT=$(jq -re '."slurm-suspend"' <<< "$META_DEVEL"); then
            		echo "INFO: updating suspend.py from project metadata"
            		printf '%s' "$SUSPEND_SCRIPT" > $SUSPEND_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-suspend not found in project metadata, skipping update"
            	fi
            	if SLURMSYNC_SCRIPT=$(jq -re '."slurmsync"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmsync.py from project metadata"
            		printf '%s' "$SLURMSYNC_SCRIPT" > $SLURMSYNC_SCRIPT_FILE
            	else
            		echo "WARNING: slurmsync not found in project metadata, skipping update"
            	fi
            	if SLURMEVENTD_SCRIPT=$(jq -re '."slurmeventd"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmeventd.py from project metadata"
            		printf '%s' "$SLURMEVENTD_SCRIPT" > $SLURMEVENTD_SCRIPT_FILE
            	else
            		echo "WARNING: slurmeventd not found in project metadata, skipping update"
            	fi
            }
            
            PING_METADATA="ping -q -w1 -c1 $METADATA_SERVER"
            echo "INFO: $PING_METADATA"
            for i in $(seq 10); do
                [ $i -gt 1 ] && sleep 5;
                $PING_METADATA > /dev/null && s=0 && break || s=$?;
                echo "ERROR: Failed to contact metadata server, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "ERROR: Unable to contact metadata server, aborting"
                wall -n '*** Slurm setup failed in the startup script! see `journalctl -u google-startup-scripts` ***'
                exit 1
            else
                echo "INFO: Successfully contacted metadata server"
            fi
            
            GOOGLE_DNS=8.8.8.8
            PING_GOOGLE="ping -q -w1 -c1 $GOOGLE_DNS"
            echo "INFO: $PING_GOOGLE"
            for i in $(seq 5); do
                [ $i -gt 1 ] && sleep 2;
                $PING_GOOGLE > /dev/null && s=0 && break || s=$?;
            	echo "failed to ping Google DNS, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "WARNING: No internet access detected"
            else
                echo "INFO: Internet access detected"
            fi
            
            mkdir -p $SCRIPTS_DIR
            
            STARTUP_SCRIPT_FILE=$SCRIPTS_DIR/startup.sh
            SETUP_SCRIPT_FILE=$SCRIPTS_DIR/setup.py
            UTIL_SCRIPT_FILE=$SCRIPTS_DIR/util.py
            RESUME_SCRIPT_FILE=$SCRIPTS_DIR/resume.py
            SUSPEND_SCRIPT_FILE=$SCRIPTS_DIR/suspend.py
            SLURMSYNC_SCRIPT_FILE=$SCRIPTS_DIR/slurmsync.py
            SLURMEVENTD_SCRIPT_FILE=$SCRIPTS_DIR/slurmeventd.py
            fetch_scripts
            
            if [ -f $FLAGFILE ]; then
            	echo "WARNING: Slurm was previously configured, quitting"
            	exit 0
            fi
            touch $FLAGFILE
            
            function fetch_feature {
            	if slurmd_feature="$($CURL $URL/instance/attributes/slurmd_feature)"; then
            		echo "$slurmd_feature"
            	else
            		echo ""
            	fi
            }
            SLURMD_FEATURE="$(fetch_feature)"
            
            echo "INFO: Running python cluster setup script"
            chmod +x $SETUP_SCRIPT_FILE
            python3 $SCRIPTS_DIR/util.py
            if [[ -n "$SLURMD_FEATURE" ]]; then
            	echo "INFO: Running dynamic node setup."
            	exec $SETUP_SCRIPT_FILE --slurmd-feature="$SLURMD_FEATURE"
            else
            	exec $SETUP_SCRIPT_FILE
            fi
        EOT
      + name                    = (known after apply)
      + name_prefix             = "clustera0d-controller-default-"
      + project                 = "eimantask-personal-project"
      + region                  = "us-east1"
      + self_link               = (known after apply)
      + self_link_unique        = (known after apply)
      + tags                    = [
          + "clustera0d",
        ]
      + tags_fingerprint        = (known after apply)

      + advanced_machine_features {
          + enable_nested_virtualization = false
          + threads_per_core             = 1
        }

      + confidential_instance_config {
          + enable_confidential_compute = false
        }

      + disk {
          + auto_delete      = true
          + boot             = true
          + device_name      = (known after apply)
          + disk_size_gb     = 50
          + disk_type        = "pd-standard"
          + interface        = (known after apply)
          + labels           = {
              + "created_by"          = "aicluster-server"
              + "ghpc_blueprint"      = "cluster-a0d34405"
              + "ghpc_deployment"     = "cluster-a0d34405"
              + "ghpc_module"         = "schedmd-slurm-gcp-v5-controller"
              + "ghpc_role"           = "scheduler"
              + "slurm_cluster_name"  = "clustera0d"
              + "slurm_instance_role" = "controller"
            }
          + mode             = (known after apply)
          + provisioned_iops = (known after apply)
          + source_image     = "projects/schedmd-slurm-public/global/images/family/slurm-gcp-5-10-hpc-centos-7"
          + type             = "PERSISTENT"
        }

      + network_interface {
          + internal_ipv6_prefix_length = (known after apply)
          + ipv6_access_type            = (known after apply)
          + ipv6_address                = (known after apply)
          + name                        = (known after apply)
          + network                     = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/global/networks/legible-polliwog-network"
          + stack_type                  = (known after apply)
          + subnetwork                  = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2"
          + subnetwork_project          = (known after apply)
        }

      + network_performance_config {
          + total_egress_bandwidth_tier = "DEFAULT"
        }

      + scheduling {
          + automatic_restart   = true
          + on_host_maintenance = "MIGRATE"
          + preemptible         = false
          + provisioning_model  = (known after apply)
        }

      + service_account {
          + email  = (known after apply)
          + scopes = [
              + "https://www.googleapis.com/auth/cloud-platform",
              + "https://www.googleapis.com/auth/devstorage.read_write",
              + "https://www.googleapis.com/auth/logging.write",
              + "https://www.googleapis.com/auth/monitoring.write",
              + "https://www.googleapis.com/auth/pubsub",
            ]
        }
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.data.google_compute_instance_template.base will be read during apply
  # (config refers to values not yet known)
 <= data "google_compute_instance_template" "base" {
      + advanced_machine_features    = (known after apply)
      + can_ip_forward               = (known after apply)
      + confidential_instance_config = (known after apply)
      + description                  = (known after apply)
      + disk                         = (known after apply)
      + guest_accelerator            = (known after apply)
      + id                           = (known after apply)
      + instance_description         = (known after apply)
      + labels                       = (known after apply)
      + machine_type                 = (known after apply)
      + metadata                     = (known after apply)
      + metadata_fingerprint         = (known after apply)
      + metadata_startup_script      = (known after apply)
      + min_cpu_platform             = (known after apply)
      + name                         = (known after apply)
      + name_prefix                  = (known after apply)
      + network_interface            = (known after apply)
      + network_performance_config   = (known after apply)
      + project                      = "eimantask-personal-project"
      + region                       = (known after apply)
      + reservation_affinity         = (known after apply)
      + resource_policies            = (known after apply)
      + scheduling                   = (known after apply)
      + self_link                    = (known after apply)
      + service_account              = (known after apply)
      + shielded_instance_config     = (known after apply)
      + tags                         = (known after apply)
      + tags_fingerprint             = (known after apply)
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.data.google_compute_zones.available will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "google_compute_zones" "available" {
      + id      = (known after apply)
      + names   = (known after apply)
      + project = "eimantask-personal-project"
      + region  = "us-east1"
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.data.local_file.startup will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "local_file" "startup" {
      + content              = (known after apply)
      + content_base64       = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + filename             = "/opt/gcluster/hpc-toolkit/community/front-end/ofe/clusters/cluster_3/cluster-a0d34405/primary/.terraform/modules/slurm_login.slurm_login_instance/scripts/startup.sh"
      + id                   = (known after apply)
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.google_compute_instance_from_template.slurm_instance[0] will be created
  + resource "google_compute_instance_from_template" "slurm_instance" {
      + allow_stopping_for_update = true
      + attached_disk             = (known after apply)
      + can_ip_forward            = (known after apply)
      + cpu_platform              = (known after apply)
      + current_status            = (known after apply)
      + deletion_protection       = (known after apply)
      + description               = (known after apply)
      + desired_status            = (known after apply)
      + enable_display            = (known after apply)
      + guest_accelerator         = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + instance_id               = (known after apply)
      + label_fingerprint         = (known after apply)
      + labels                    = (known after apply)
      + machine_type              = (known after apply)
      + metadata                  = (known after apply)
      + metadata_fingerprint      = (known after apply)
      + metadata_startup_script   = (known after apply)
      + min_cpu_platform          = (known after apply)
      + name                      = (known after apply)
      + project                   = "eimantask-personal-project"
      + resource_policies         = (known after apply)
      + scratch_disk              = (known after apply)
      + self_link                 = (known after apply)
      + service_account           = (known after apply)
      + source_instance_template  = (known after apply)
      + tags                      = (known after apply)
      + tags_fingerprint          = (known after apply)
      + zone                      = "us-east1-b"

      + network_interface {
          + access_config               = (known after apply)
          + alias_ip_range              = (known after apply)
          + internal_ipv6_prefix_length = (known after apply)
          + ipv6_access_type            = (known after apply)
          + ipv6_address                = (known after apply)
          + name                        = (known after apply)
          + network                     = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/global/networks/legible-polliwog-network"
          + network_ip                  = (known after apply)
          + nic_type                    = (known after apply)
          + queue_count                 = (known after apply)
          + stack_type                  = (known after apply)
          + subnetwork                  = "legible-polliwog-subnet-2"
          + subnetwork_project          = (known after apply)
        }
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.google_pubsub_subscription.pull_subscriptions[0] will be created
  + resource "google_pubsub_subscription" "pull_subscriptions" {
      + ack_deadline_seconds       = 120
      + enable_message_ordering    = true
      + id                         = (known after apply)
      + labels                     = {
          + "slurm_cluster_name" = "clustera0d"
        }
      + message_retention_duration = "604800s"
      + name                       = (known after apply)
      + project                    = "eimantask-personal-project"
      + topic                      = (known after apply)

      + retry_policy {
          + maximum_backoff = "300s"
          + minimum_backoff = "30s"
        }
    }

  # module.slurm_login.module.slurm_login_instance.module.slurm_login_instance.google_pubsub_subscription_iam_member.pull_subscription_binding[0] will be created
  + resource "google_pubsub_subscription_iam_member" "pull_subscription_binding" {
      + etag         = (known after apply)
      + id           = (known after apply)
      + member       = (known after apply)
      + project      = "eimantask-personal-project"
      + role         = "roles/pubsub.subscriber"
      + subscription = (known after apply)
    }

  # module.slurm_login.module.slurm_login_template.module.instance_template.google_compute_instance_template.tpl will be created
  + resource "google_compute_instance_template" "tpl" {
      + can_ip_forward          = false
      + id                      = (known after apply)
      + labels                  = {
          + "created_by"          = "aicluster-server"
          + "ghpc_blueprint"      = "cluster-a0d34405"
          + "ghpc_deployment"     = "cluster-a0d34405"
          + "ghpc_module"         = "schedmd-slurm-gcp-v5-login"
          + "ghpc_role"           = "scheduler"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "login"
        }
      + machine_type            = "n2-standard-2"
      + metadata                = {
          + "VmDnsSetting"        = "GlobalOnly"
          + "enable-oslogin"      = "TRUE"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "login"
        }
      + metadata_fingerprint    = (known after apply)
      + metadata_startup_script = <<-EOT
            #!/bin/bash
            # Copyright (C) SchedMD LLC.
            #
            # Licensed under the Apache License, Version 2.0 (the "License");
            # you may not use this file except in compliance with the License.
            # You may obtain a copy of the License at
            #
            #     http://www.apache.org/licenses/LICENSE-2.0
            #
            # Unless required by applicable law or agreed to in writing, software
            # distributed under the License is distributed on an "AS IS" BASIS,
            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            # See the License for the specific language governing permissions and
            # limitations under the License.
            
            set -e
            
            SLURM_DIR=/slurm
            FLAGFILE=$SLURM_DIR/slurm_configured_do_not_remove
            SCRIPTS_DIR=$SLURM_DIR/scripts
            
            METADATA_SERVER="metadata.google.internal"
            URL="http://$METADATA_SERVER/computeMetadata/v1"
            HEADER="Metadata-Flavor:Google"
            CURL="curl -sS --fail --header $HEADER"
            
            function fetch_scripts {
            	# fetch project metadata
            	if ! CLUSTER=$($CURL $URL/instance/attributes/slurm_cluster_name); then
            		echo "ERROR: cluster name not found in instance metadata. Quitting!"
            		return 1
            	fi
            	if ! META_DEVEL=$($CURL $URL/project/attributes/$CLUSTER-slurm-devel); then
            		echo "WARNING: $CLUSTER-slurm-devel not found in project metadata, skipping script update"
            		return
            	fi
            	echo devel data found in project metadata, looking to update scripts
            	if STARTUP_SCRIPT=$(jq -re '."startup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating startup.sh from project metadata"
            		printf '%s' "$STARTUP_SCRIPT" > $STARTUP_SCRIPT_FILE
            	else
            		echo "WARNING: startup-script not found in project metadata, skipping update"
            	fi
            	if SETUP_SCRIPT=$(jq -re '."setup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating setup.py from project metadata"
            		printf '%s' "$SETUP_SCRIPT" > $SETUP_SCRIPT_FILE
            	else
            		echo "WARNING: setup-script not found in project metadata, skipping update"
            	fi
            	if UTIL_SCRIPT=$(jq -re '."util-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating util.py from project metadata"
            		printf '%s' "$UTIL_SCRIPT" > $UTIL_SCRIPT_FILE
            	else
            		echo "WARNING: util-script not found in project metadata, skipping update"
            	fi
            	if RESUME_SCRIPT=$(jq -re '."slurm-resume"' <<< "$META_DEVEL"); then
            		echo "INFO: updating resume.py from project metadata"
            		printf '%s' "$RESUME_SCRIPT" > $RESUME_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-resume not found in project metadata, skipping update"
            	fi
            	if SUSPEND_SCRIPT=$(jq -re '."slurm-suspend"' <<< "$META_DEVEL"); then
            		echo "INFO: updating suspend.py from project metadata"
            		printf '%s' "$SUSPEND_SCRIPT" > $SUSPEND_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-suspend not found in project metadata, skipping update"
            	fi
            	if SLURMSYNC_SCRIPT=$(jq -re '."slurmsync"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmsync.py from project metadata"
            		printf '%s' "$SLURMSYNC_SCRIPT" > $SLURMSYNC_SCRIPT_FILE
            	else
            		echo "WARNING: slurmsync not found in project metadata, skipping update"
            	fi
            	if SLURMEVENTD_SCRIPT=$(jq -re '."slurmeventd"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmeventd.py from project metadata"
            		printf '%s' "$SLURMEVENTD_SCRIPT" > $SLURMEVENTD_SCRIPT_FILE
            	else
            		echo "WARNING: slurmeventd not found in project metadata, skipping update"
            	fi
            }
            
            PING_METADATA="ping -q -w1 -c1 $METADATA_SERVER"
            echo "INFO: $PING_METADATA"
            for i in $(seq 10); do
                [ $i -gt 1 ] && sleep 5;
                $PING_METADATA > /dev/null && s=0 && break || s=$?;
                echo "ERROR: Failed to contact metadata server, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "ERROR: Unable to contact metadata server, aborting"
                wall -n '*** Slurm setup failed in the startup script! see `journalctl -u google-startup-scripts` ***'
                exit 1
            else
                echo "INFO: Successfully contacted metadata server"
            fi
            
            GOOGLE_DNS=8.8.8.8
            PING_GOOGLE="ping -q -w1 -c1 $GOOGLE_DNS"
            echo "INFO: $PING_GOOGLE"
            for i in $(seq 5); do
                [ $i -gt 1 ] && sleep 2;
                $PING_GOOGLE > /dev/null && s=0 && break || s=$?;
            	echo "failed to ping Google DNS, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "WARNING: No internet access detected"
            else
                echo "INFO: Internet access detected"
            fi
            
            mkdir -p $SCRIPTS_DIR
            
            STARTUP_SCRIPT_FILE=$SCRIPTS_DIR/startup.sh
            SETUP_SCRIPT_FILE=$SCRIPTS_DIR/setup.py
            UTIL_SCRIPT_FILE=$SCRIPTS_DIR/util.py
            RESUME_SCRIPT_FILE=$SCRIPTS_DIR/resume.py
            SUSPEND_SCRIPT_FILE=$SCRIPTS_DIR/suspend.py
            SLURMSYNC_SCRIPT_FILE=$SCRIPTS_DIR/slurmsync.py
            SLURMEVENTD_SCRIPT_FILE=$SCRIPTS_DIR/slurmeventd.py
            fetch_scripts
            
            if [ -f $FLAGFILE ]; then
            	echo "WARNING: Slurm was previously configured, quitting"
            	exit 0
            fi
            touch $FLAGFILE
            
            function fetch_feature {
            	if slurmd_feature="$($CURL $URL/instance/attributes/slurmd_feature)"; then
            		echo "$slurmd_feature"
            	else
            		echo ""
            	fi
            }
            SLURMD_FEATURE="$(fetch_feature)"
            
            echo "INFO: Running python cluster setup script"
            chmod +x $SETUP_SCRIPT_FILE
            python3 $SCRIPTS_DIR/util.py
            if [[ -n "$SLURMD_FEATURE" ]]; then
            	echo "INFO: Running dynamic node setup."
            	exec $SETUP_SCRIPT_FILE --slurmd-feature="$SLURMD_FEATURE"
            else
            	exec $SETUP_SCRIPT_FILE
            fi
        EOT
      + name                    = (known after apply)
      + name_prefix             = "clustera0d-login-default-"
      + project                 = "eimantask-personal-project"
      + region                  = "us-east1"
      + self_link               = (known after apply)
      + self_link_unique        = (known after apply)
      + tags                    = [
          + "clustera0d",
        ]
      + tags_fingerprint        = (known after apply)

      + advanced_machine_features {
          + enable_nested_virtualization = false
          + threads_per_core             = 1
        }

      + confidential_instance_config {
          + enable_confidential_compute = false
        }

      + disk {
          + auto_delete      = true
          + boot             = true
          + device_name      = (known after apply)
          + disk_size_gb     = 50
          + disk_type        = "pd-standard"
          + interface        = (known after apply)
          + labels           = {
              + "created_by"          = "aicluster-server"
              + "ghpc_blueprint"      = "cluster-a0d34405"
              + "ghpc_deployment"     = "cluster-a0d34405"
              + "ghpc_module"         = "schedmd-slurm-gcp-v5-login"
              + "ghpc_role"           = "scheduler"
              + "slurm_cluster_name"  = "clustera0d"
              + "slurm_instance_role" = "login"
            }
          + mode             = (known after apply)
          + provisioned_iops = (known after apply)
          + source_image     = "projects/schedmd-slurm-public/global/images/family/slurm-gcp-5-10-hpc-centos-7"
          + type             = "PERSISTENT"
        }

      + network_interface {
          + internal_ipv6_prefix_length = (known after apply)
          + ipv6_access_type            = (known after apply)
          + ipv6_address                = (known after apply)
          + name                        = (known after apply)
          + network                     = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/global/networks/legible-polliwog-network"
          + stack_type                  = (known after apply)
          + subnetwork                  = "legible-polliwog-subnet-2"
          + subnetwork_project          = (known after apply)
        }

      + network_performance_config {
          + total_egress_bandwidth_tier = "DEFAULT"
        }

      + scheduling {
          + automatic_restart   = true
          + on_host_maintenance = "MIGRATE"
          + preemptible         = false
          + provisioning_model  = (known after apply)
        }

      + service_account {
          + email  = (known after apply)
          + scopes = [
              + "https://www.googleapis.com/auth/cloud-platform",
              + "https://www.googleapis.com/auth/devstorage.read_write",
              + "https://www.googleapis.com/auth/logging.write",
              + "https://www.googleapis.com/auth/monitoring.write",
            ]
        }
    }

  # module.partition_0.module.slurm_partition.module.slurm_compute_template["ghpc"].module.instance_template.google_compute_instance_template.tpl will be created
  + resource "google_compute_instance_template" "tpl" {
      + can_ip_forward          = false
      + id                      = (known after apply)
      + labels                  = {
          + "created_by"          = "aicluster-server"
          + "ghpc_blueprint"      = "cluster-a0d34405"
          + "ghpc_deployment"     = "cluster-a0d34405"
          + "ghpc_module"         = "schedmd-slurm-gcp-v5-node-group"
          + "ghpc_role"           = "compute"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "compute"
        }
      + machine_type            = "c2-standard-60"
      + metadata                = {
          + "VmDnsSetting"        = "GlobalOnly"
          + "enable-oslogin"      = "TRUE"
          + "slurm_cluster_name"  = "clustera0d"
          + "slurm_instance_role" = "compute"
        }
      + metadata_fingerprint    = (known after apply)
      + metadata_startup_script = <<-EOT
            #!/bin/bash
            # Copyright (C) SchedMD LLC.
            #
            # Licensed under the Apache License, Version 2.0 (the "License");
            # you may not use this file except in compliance with the License.
            # You may obtain a copy of the License at
            #
            #     http://www.apache.org/licenses/LICENSE-2.0
            #
            # Unless required by applicable law or agreed to in writing, software
            # distributed under the License is distributed on an "AS IS" BASIS,
            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            # See the License for the specific language governing permissions and
            # limitations under the License.
            
            set -e
            
            SLURM_DIR=/slurm
            FLAGFILE=$SLURM_DIR/slurm_configured_do_not_remove
            SCRIPTS_DIR=$SLURM_DIR/scripts
            
            METADATA_SERVER="metadata.google.internal"
            URL="http://$METADATA_SERVER/computeMetadata/v1"
            HEADER="Metadata-Flavor:Google"
            CURL="curl -sS --fail --header $HEADER"
            
            function fetch_scripts {
            	# fetch project metadata
            	if ! CLUSTER=$($CURL $URL/instance/attributes/slurm_cluster_name); then
            		echo "ERROR: cluster name not found in instance metadata. Quitting!"
            		return 1
            	fi
            	if ! META_DEVEL=$($CURL $URL/project/attributes/$CLUSTER-slurm-devel); then
            		echo "WARNING: $CLUSTER-slurm-devel not found in project metadata, skipping script update"
            		return
            	fi
            	echo devel data found in project metadata, looking to update scripts
            	if STARTUP_SCRIPT=$(jq -re '."startup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating startup.sh from project metadata"
            		printf '%s' "$STARTUP_SCRIPT" > $STARTUP_SCRIPT_FILE
            	else
            		echo "WARNING: startup-script not found in project metadata, skipping update"
            	fi
            	if SETUP_SCRIPT=$(jq -re '."setup-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating setup.py from project metadata"
            		printf '%s' "$SETUP_SCRIPT" > $SETUP_SCRIPT_FILE
            	else
            		echo "WARNING: setup-script not found in project metadata, skipping update"
            	fi
            	if UTIL_SCRIPT=$(jq -re '."util-script"' <<< "$META_DEVEL"); then
            		echo "INFO: updating util.py from project metadata"
            		printf '%s' "$UTIL_SCRIPT" > $UTIL_SCRIPT_FILE
            	else
            		echo "WARNING: util-script not found in project metadata, skipping update"
            	fi
            	if RESUME_SCRIPT=$(jq -re '."slurm-resume"' <<< "$META_DEVEL"); then
            		echo "INFO: updating resume.py from project metadata"
            		printf '%s' "$RESUME_SCRIPT" > $RESUME_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-resume not found in project metadata, skipping update"
            	fi
            	if SUSPEND_SCRIPT=$(jq -re '."slurm-suspend"' <<< "$META_DEVEL"); then
            		echo "INFO: updating suspend.py from project metadata"
            		printf '%s' "$SUSPEND_SCRIPT" > $SUSPEND_SCRIPT_FILE
            	else
            		echo "WARNING: slurm-suspend not found in project metadata, skipping update"
            	fi
            	if SLURMSYNC_SCRIPT=$(jq -re '."slurmsync"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmsync.py from project metadata"
            		printf '%s' "$SLURMSYNC_SCRIPT" > $SLURMSYNC_SCRIPT_FILE
            	else
            		echo "WARNING: slurmsync not found in project metadata, skipping update"
            	fi
            	if SLURMEVENTD_SCRIPT=$(jq -re '."slurmeventd"' <<< "$META_DEVEL"); then
            		echo "INFO: updating slurmeventd.py from project metadata"
            		printf '%s' "$SLURMEVENTD_SCRIPT" > $SLURMEVENTD_SCRIPT_FILE
            	else
            		echo "WARNING: slurmeventd not found in project metadata, skipping update"
            	fi
            }
            
            PING_METADATA="ping -q -w1 -c1 $METADATA_SERVER"
            echo "INFO: $PING_METADATA"
            for i in $(seq 10); do
                [ $i -gt 1 ] && sleep 5;
                $PING_METADATA > /dev/null && s=0 && break || s=$?;
                echo "ERROR: Failed to contact metadata server, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "ERROR: Unable to contact metadata server, aborting"
                wall -n '*** Slurm setup failed in the startup script! see `journalctl -u google-startup-scripts` ***'
                exit 1
            else
                echo "INFO: Successfully contacted metadata server"
            fi
            
            GOOGLE_DNS=8.8.8.8
            PING_GOOGLE="ping -q -w1 -c1 $GOOGLE_DNS"
            echo "INFO: $PING_GOOGLE"
            for i in $(seq 5); do
                [ $i -gt 1 ] && sleep 2;
                $PING_GOOGLE > /dev/null && s=0 && break || s=$?;
            	echo "failed to ping Google DNS, will retry"
            done
            if [ $s -ne 0 ]; then
                echo "WARNING: No internet access detected"
            else
                echo "INFO: Internet access detected"
            fi
            
            mkdir -p $SCRIPTS_DIR
            
            STARTUP_SCRIPT_FILE=$SCRIPTS_DIR/startup.sh
            SETUP_SCRIPT_FILE=$SCRIPTS_DIR/setup.py
            UTIL_SCRIPT_FILE=$SCRIPTS_DIR/util.py
            RESUME_SCRIPT_FILE=$SCRIPTS_DIR/resume.py
            SUSPEND_SCRIPT_FILE=$SCRIPTS_DIR/suspend.py
            SLURMSYNC_SCRIPT_FILE=$SCRIPTS_DIR/slurmsync.py
            SLURMEVENTD_SCRIPT_FILE=$SCRIPTS_DIR/slurmeventd.py
            fetch_scripts
            
            if [ -f $FLAGFILE ]; then
            	echo "WARNING: Slurm was previously configured, quitting"
            	exit 0
            fi
            touch $FLAGFILE
            
            function fetch_feature {
            	if slurmd_feature="$($CURL $URL/instance/attributes/slurmd_feature)"; then
            		echo "$slurmd_feature"
            	else
            		echo ""
            	fi
            }
            SLURMD_FEATURE="$(fetch_feature)"
            
            echo "INFO: Running python cluster setup script"
            chmod +x $SETUP_SCRIPT_FILE
            python3 $SCRIPTS_DIR/util.py
            if [[ -n "$SLURMD_FEATURE" ]]; then
            	echo "INFO: Running dynamic node setup."
            	exec $SETUP_SCRIPT_FILE --slurmd-feature="$SLURMD_FEATURE"
            else
            	exec $SETUP_SCRIPT_FILE
            fi
        EOT
      + name                    = (known after apply)
      + name_prefix             = "clustera0d-compute-batch-ghpc-"
      + project                 = "eimantask-personal-project"
      + region                  = (known after apply)
      + self_link               = (known after apply)
      + self_link_unique        = (known after apply)
      + tags                    = [
          + "clustera0d",
        ]
      + tags_fingerprint        = (known after apply)

      + advanced_machine_features {
          + enable_nested_virtualization = false
          + threads_per_core             = 1
        }

      + confidential_instance_config {
          + enable_confidential_compute = false
        }

      + disk {
          + auto_delete      = true
          + boot             = true
          + device_name      = (known after apply)
          + disk_size_gb     = 50
          + disk_type        = "pd-standard"
          + interface        = (known after apply)
          + labels           = {
              + "created_by"          = "aicluster-server"
              + "ghpc_blueprint"      = "cluster-a0d34405"
              + "ghpc_deployment"     = "cluster-a0d34405"
              + "ghpc_module"         = "schedmd-slurm-gcp-v5-node-group"
              + "ghpc_role"           = "compute"
              + "slurm_cluster_name"  = "clustera0d"
              + "slurm_instance_role" = "compute"
            }
          + mode             = (known after apply)
          + provisioned_iops = (known after apply)
          + source_image     = "projects/schedmd-slurm-public/global/images/family/slurm-gcp-5-10-hpc-centos-7"
          + type             = "PERSISTENT"
        }

      + network_interface {
          + internal_ipv6_prefix_length = (known after apply)
          + ipv6_access_type            = (known after apply)
          + ipv6_address                = (known after apply)
          + name                        = (known after apply)
          + network                     = (known after apply)
          + stack_type                  = (known after apply)
          + subnetwork                  = "https://www.googleapis.com/compute/v1/projects/eimantask-personal-project/regions/us-east1/subnetworks/legible-polliwog-subnet-2"
          + subnetwork_project          = (known after apply)
        }

      + network_performance_config {
          + total_egress_bandwidth_tier = "DEFAULT"
        }

      + scheduling {
          + automatic_restart   = true
          + on_host_maintenance = "TERMINATE"
          + preemptible         = false
          + provisioning_model  = (known after apply)
        }

      + service_account {
          + email  = "260846834422-compute@developer.gserviceaccount.com"
          + scopes = [
              + "https://www.googleapis.com/auth/cloud-platform",
            ]
        }
    }

Plan: 42 to add, 0 to change, 0 to destroy.

Warning: Argument is deprecated

  with module.slurm_controller.module.slurm_controller_instance.google_secret_manager_secret.cloudsql,
  on .terraform/modules/slurm_controller.slurm_controller_instance/terraform/slurm_cluster/modules/slurm_controller_instance/main.tf line 399, in resource "google_secret_manager_secret" "cloudsql":
 399:     automatic = true

`automatic` is deprecated and will be removed in a future major release. Use
`auto` instead.

─────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't
guarantee to take exactly these actions if you run "terraform apply" now.
